# 感知机模型(Perceptron model)
## 感知机模型的概念
感知机是二类分类的线性分类模型, 其输入为实例的特征向量, 输出为实例的类别, 取+1和-1值
感知机对应与输入空间中讲实例划分为正负两类的分割超平面, 输入判别模型

旨在求出将训练数据进行线性可分的分离超平面
导入基于误分类的损失函数, 利用梯度下降法对损失函数进行极小化,
简单而易于实现, 分为原始形式和对偶形式
感知机是神经网络与支持向量机的基础

## 感知机学习策略
1. 首先输入空间任意一点$x_0$到超平面S的距离为:
$\frac{1}{||w||} {|w * x_0 +b |}$
2. 其次对于误分类的数据$(x_i, y_i)$来说:
 $-y_i(w * x_i + b)$
3. 因此感知机$sign(w*x + b)$学习的损失函数定义为:$L(w, b) = -\sum\limits_{x_i \in M} y_i(w * x_i +b)$
其中: M为误分类点的集合, 这个损失函数就是感知机学习的经验风险函数

## 感知机学习算法
### 感知机学习算法的原始形式
### 原始形式实现逻辑
输入: 训练数集$T={(x_i, y_i), (x_2, y_2), ..., (x_n, y_n)}$,其中$x_i \in \chi = R^n, y_i \in Y = {+1, -1}, i=1,2,3,...,N, 学习率 n(0<n<1)$:
输出: w, b 感知机模型f(x)=sign(w*x + b)
(1) 选取初始值$w_0, b_0$
(2) 在训练集中选取数据(x_i , y_i)
(3) 如果y_i(w*x_i + b) <= 0
    w = w + ny_ix_i
    b = b + ny_i
(4) 转至(2): 知道训练集中没有误分类数据

### 感知机学习算法的对偶形式
对偶形式的基本思想: 将w, b表示为实例x_i和标记y_i的线性组合形式, 通过求解其系数而求得w和b.

在感知机学习算法的原始形式中:
    $w <- w + ny_ix_i \\ b <- b + ny_i$
 
逐步修改w, b, 设修改n次, 则w, b关于(x_i, y_i)的增量分别为$\alpha_iy_ix_i和\alpha_iy_i$,
这里$\alpha_i = n_i n$
最后学习到的w, b可以分别表示为:
    $w = \sum\limits_{i=1}^{N} \alpha_i y_i x_i$
    $b = \sum\limits_{i=1}^{N} \alpha_i y_i$

### 对偶形式实现逻辑
输入: 训练数集$T={(x_i, y_i), (x_2, y_2), ..., (x_n, y_n)}$,其中$x_i \in \chi = R^n, y_i \in Y = {+1, -1}, i=1,2,3,...,N, 学习率 n(0<n<1)$:
输出: $\alpha, b$ 感知机模型:$f(x)=sign(\sum\limits_{j=1}^{N}\alpha_jy_jx_j*x + b)$
其中$\alpha=(\alpha_i, \alpha_2, ... , \alpha_N)^T$
(1) alpha <- 0, b <- 0
(2) 在训练数据中选取数据(x_i, y_i)
(3) 如果y_i(\sum\limits_{j=1}^{N}\alpha_jy_jx_j * x_i + b) \leq 0$:
    \alpha_i <- \alpha_i + n
    b <- b + ny_i
(4) 转至2 直到没有误分类数据
对偶形式中训练实例仅以内积的形式出现,  可以预先将数据集中实例间的内积计算出来并以矩阵的形式存储
这个矩阵就是所谓的GRAM矩阵
G = [x_i ,x_j]_{N x N}

