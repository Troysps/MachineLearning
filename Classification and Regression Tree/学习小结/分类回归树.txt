Classification And Regression Trees 分类与回归树CART
分类回归树: 面对非线性问题
CART：
		每个叶节点上构建一个线性模型 
		误差计算是以:线性模型拟合计算
regression Tree:	
		每个叶节点上都是以'常量'  
		误差计算是以: 总方差计算
		
决策树算法: ID3,ID4.5,GINI指数
	决策树算法实质上是一种贪心算法, 不断将数据集切分为小数据集, 直到所有目标变量完全相同,
	或者到数据不能再切分为止
这种情况下,并不关心是否达到全局最优

树构建算法----两种基本思路
(1)ID3算法, 每次选取当前最佳特征来分隔数据,  并按照该特征的所有可能取值来切分数据,
一旦切分完成,该特征不再考虑
(2)二元切分法,即每次把数据集切分为两份,如果数据的某些特征值等于切分所要求的值,那么进入右子树,反之进入左子树


连续和离散型特征的树的构建 --- 解决多种类型数据的存储问题
利用字典来存储树的数据结构
该字典将包含以下4个元素
	1.带切分的特征
	2.带切分的特征值
	3.右子树
	4.左子树
	
CART:分类与回归树
本次算法将构建两种树：
回归树:regression tree 
模型树:model tree


创建树流程及主要函数
step1: load data		读取数据集
step2: bin_split_data	二元切分数据集
step3: create_tree		构建树
	
function create_tree()伪代码
```
找到最佳特征与特征值
	如果该节点不能切分,将该节点储存为叶节点
	反之:执行二元切分法
	右子树调用: create_tree()
	左子树调用: create_tree()
```

问题: 如何找到最佳特征与特征值
choose_best_feature() 选择最佳切分的特征与阈值伪代码流程
```
对每个特征
	对每个特征值
		将数据集且分为两份
		计算切分的误差
		如果当前误差小于当前最小误差,那么将当前切分设置为最佳切分值并更新最小误差
返回最佳切分的特征
```

CART算法用于回归
问题: 
	如何实现数据的切分?
	怎么才知道是否已经充分切分
解决:
	取决于叶节点的建模方式
	回归树假设叶节点是常数值
	
regression tree: 以分段常数为叶节点的树 需要度量出数据的一致性
如何度量数据的一致性:
	分类树:信息熵 信息增益 信息增益率 基尼指数
	回归树:
		计算所有数据的均值
		计算每条数据的值到均值的差值
		对正负差值同等看待 使用绝对值或平方值代替上述差值
		在统计学中:方差是平方误差的均值
		这里是平方误差的总值(总方差)： 均方差乘以数据集中样本点个数
		
构建树
	原理: 
		给定某个误差计算方法
		找到数据集上最佳的二元切分方式
		确定什么时候停止切分
		一旦停止切分就生成一个叶节点
		
函数choose_best_feature():
	解决两个问题:	
				(1)用最佳方式切分数据集
				(2)生成相应的叶节点
				
因此在函数choose_best_feature()中存在四个参数
	param:data_set 	数据集
	param:type_leaf 叶节点计算方式
	param:type_err 	误差计算方式
	param:ops		用户定义参数构成的元组(limit_error, limit_length)
					limit_error:限制预测结果与真实结果之间的误差
					limit_length:限制左右子树 避免出现过拟合
					

树剪枝
原因:一棵树,如果节点过多,表明该模型可能对数据进行了过拟合
如何判断是否发生了过拟合?
采用某种交叉验证技术来发现过拟合

树剪枝(pruning):降低决策树的复杂度来避免过拟合的过程称为剪枝
剪枝分为预剪枝与后剪枝
预剪枝
在选择最优特征与特征值的函数中参数ops=(limit_error, limit_length)就是预剪枝
预剪枝为ops()中的参数敏感
用户定义参数构成的元组,用户需要设置参数,通过不断修改停止条件来得到合理结构并不是很好的方法

后剪枝
需要将数据集分为测试集与训练集
要点:
	(1)首先构建出足够复杂的树(假设该树是过拟合)
	(2)从上而下找出叶节点,用测试集判断将这些叶节点合并能否降低误差
	(3)如果是的话,就合并
	
后剪枝函数prune()伪代码:
```
基于已有的树切分测试集
	如果存在任一子集是一棵树,则在该子集递归剪枝过程
	计算将当前两个叶节点合并后的误差
	计算不合并的误差
	如果合并为降低误差的话,将该叶节点合并
```

完成剪枝需要三个函数
	(1)判断是否为树
	(2)递归直到叶节点为止,如果找到两个叶节点,则计算它们的平均值,对树进行塌陷处理(即:返回树的平均值)
	(3)主函数 prune()中有两个参数: 待剪枝的树与剪枝所需要的测试数据集
	
具体步骤:
	step1:首先需要确认测试集是否为空
	step2:一旦非空,则反复递归调用函数对测试数据进行切分
	step3:
		检查某个分支到底是子树还是节点
		如果是子树,就调用函数prune来对该子树进行剪枝
		对左右两个分支完成剪枝后,需要检查是否仍然是子树
		不再是子树,就可以进行合并
		对合并前后误差进行比较
		

	
模型树:把叶节点设定为分段线性函数
分段线性:是指模型由多个线性片段组成
优点:
	比回归更加容易理解
	有更高的预测率
	
算法简述:利用树生成算法对数据进行切分	且每份数据能很容易被线性模型所表示	该算法关键在于误差计算
模型树中,如何找到最佳切分
误差计算方法:
	(1)对于给定的数据集, 应该先用线性模型来拟合
	(2)然后计算真实的目标值与模型预测间的差值
	(3)误差为这些差值的平方和

regression tree and model tree区别
回归树:
	叶节点计算:
		return val = np.mean(data_set[:,-1])
	误差度量:
		return error=np.var(data_set[:, -1]) * np.shape(data_set)[0]
模型树:
	叶节点计算: 
		return ws  线性模型参数
	误差度量:
		线性模型的拟合计算

树回归与标准回归比较
将cart模型与标准回归模型比较
一般来说,进行相关系数进行比较  计算R^2 皮尔逊相关系数

给定树的情况下, 对单个数据点, 该函数会给出一个预测值

定义函数treeForeCast():自顶向下遍历整颗树, 知道命中叶子节点为止



