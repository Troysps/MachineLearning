集成方法
概念: 对其他算法进行组合的一种形式
集成方法:
	(1)投票学习(bagging:自举汇聚法, bootstrap aggregating):基于数据随机抽样分类器构造的方法
	(2)再学习(boosting):基于所有分类器的加权求和
	

bagging：构建多个分类器 逐一投票 投票多的被视为分类项
例子:美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友
boosting： 基于所有分类器的加权求和 -- 设定比重不断的学习强化 获取最优分类项
例子:3个帅哥追同一个美女，第1个帅哥失败->(传授经验：姓名、家庭情况) 第2个帅哥失败->(传授经验：兴趣爱好、性格特点) 第3个帅哥成功

bagging与boosting的异同
相同点:所使用的多个分类器的类型(数据量和特征量)都是一致的
不同点:
	bagging由不同的分类器(1.数据随机化,2.特征随机化)经过训练 综合得出最多非零结果
	boosting是通过调整已有分类器错分的数据来获得新的分类器 得出目前最优的结果
	
	bagging中的分类器权重是相等的,而boosting中的分类器加权求和 所以权重并不相等
	每个权重代表的是其对应分类器在上一轮迭代中的成功度
	
	
随机森林
	随机森林指的是利用多棵树对样本进行训练并预测
	随机森林是希望搭建多个臭皮匠 希望最终的分类效果能够超过单个大师的一种算法

随机森林
	数据的随机性化
	待选特征的随机化
	
1.数据的随机性化---又放回的抽取
	为什么采用有放回的抽取?
		1.保证不同子集之间的数量级一样(不同子集/同一子集之间的元素可以重复)
		2.利用子数据集来构建决策树 将这个数据放到每个子决策树中 每个子决策树输出一个结果
		3.然后统计子决策树的投票结果 得到最终分类
2.待选特征的随机化
	1.子树从所有的待选特征中随机选取一定的特征
	2.在选取的特征中选取最优的特征
	
随机森林 算法特点
	优点: 
		1.几乎不需要输入准备
		2.可实现隐式特征选择
		3.训练速度非常快
		4.其他模型很难超越
		5.很难建立一个槽糕的随机森林模型
		
	劣势:
		1.模型大小 --黑盒子 很难解释
		
		
适用数据范围:数值型和标称型




















如何构建
	1.数据的随机性化
	2.待选特征的随机化
	
