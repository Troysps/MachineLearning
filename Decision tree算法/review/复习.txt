决策树
	--回归
	--分类
	
树形模型 ---> 特征对实例的分类
三个步骤:
	--特征选择
	--决策树生产
	--决策树的修剪
	
决策树
	--节点
		--内部节点	判断
		--叶节点	类别
	--有向边
	
递归进行分配


ID3算法
	--信息熵和信息增益
	
	数据集划分之前
	计算信息增益---
	数据集划分之后
	
	特征分类 -- 首先是对结果影响最大的特征
	
	信息复杂度减少
	
	选择一个特征后 信息增益是最大的
	
算法的3个主要部分
	特征选择: 信息增益/信息增益率/GINI指数
	---熵: 表示随机变量的不确定的
	---条件熵:在一个条件下 随机变量的不确定性
	---信息增益:熵-条件熵
	
	信息增益率 --- +惩罚
		取值数目多的属性 该项的惩罚也会变大
		
	GINI指数--Card树
		描述的是纯度
		
		
ID3算法
	从根节点开始划分 结算所有有可能特征的信息增益 选择信息增益最大的特征作为节点的特征 并由该特征的不同值构建子节点
	
	对字典递归调用 构建决策树
	直到所有特征的信息增益均很小为止
	
C4.5算法
	---使用信息增益比来进行特征选择
C4.5是ID3算法的优化

CART算法
	分类与回归树
	
	假设决策树是一个二叉树 通过递归地二分每个特征 将特征空间划分为有限个单元
	对于回归树 采用平方误差
	
三个算法的细节与区别


决策树剪枝 -- 避免过拟合
	预剪纸和后剪枝
	
	预剪枝
		在划分之前 设定一定的预值 若是比预值小 就不划分了
		信息增益是否减小 是否比预值小
		
	后剪枝--都会设置一个预值
		首先生成树
		塌陷处理
		
		是否小于---预先节点
		
	
		避免过拟合 防止分类到小特征
		
优点:
	易于理解
	人类思维契合
	模型树的形式可视化
	可以处理非数值型数据
	
缺点:
	处理不好连续变量
	不好处理变量之间存在复杂关系
	决定分类的因素取决于更多变量的复杂组合
	可规模性一般
	
	

		