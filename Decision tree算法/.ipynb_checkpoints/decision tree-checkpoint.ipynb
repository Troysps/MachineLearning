{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree（决策树算法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念及其优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是决策树？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树（Decision tree），又称判定树，是一种以树结构（二叉树及多叉树）形式来表达的预测分析的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。节点有两种类型：内部节点和叶节点  内部节点表示一个特征或属性 叶节点表示一个类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通过把实例从根节点排列到某叶子节点来分类实例\n",
    "* 叶子节点即为实例所属的分类\n",
    "* 树上每个节点说明了对实例的某个属性的测试，节点的每个后序分支对应于该属性的一个可能值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归类——对连续变量做决策树\n",
    "#### 分类树——对离散变量做决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树算法特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有监督的学习\n",
    "#### 非参数学习方法\n",
    "#### 自顶向下递归方式构造决策树\n",
    "#### 在每一步选择中都采取在当前状态最好的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树学习的算法通常是一个递归选择地选择最优算法特征 并针对该特征对训练数据进行分类，使得各个子数据集有一个最好的分类过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在决策树算法中，ID3基于信息增益作为属性选择的度量，C4.5基于信息增益率作为属性选择的度量（是ID3的一个改进，比ID3准确率高且快，可以处理连续值和有缺失值的feature），CART基于基尼指数作为属性选择的度量（使用基尼指数的划分准则，通过在每个步骤最大限度降低不纯洁度，CART能够处理孤立点以及能够对空缺值进行处理。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树算法理论基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息论———稍后做详细解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择\n",
    "#### 决策树生成：递归结构，对于模型的局部最优\n",
    "#### 决策树剪枝：缩小树结构规模，缓解过拟合，对应于模型的全局选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优点：\n",
    "+ 速度快：计算量相对较小，且容易转化成分类规则。只有沿着根节点一直向下走到叶，沿途分类的条件就能够确定唯一的一条分类的谓词\n",
    "+ 准确性高：挖掘出的分类规则准确性高，便于理解，决策树可以清晰的显示哪些字段比较重要，即可以生成可以理解的规则\n",
    "+ 可以处理连续和种类字段\n",
    "+ 不需要任何领域知识和参数假设\n",
    "+ 适合高维数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺点：\n",
    "* 对于各类别样本数据不一致的数据，信息增益偏向于哪些具有跟多数值的特征\n",
    "* 易于过拟合\n",
    "* 忽略属性之间的相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树理论基础——信息论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息论（1948 Shinnon）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息论将信息的传递看做一种统计现象进行考虑，信息包含数据流中的事件、样本、特征，事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的期望均值就是这个分布产生的信息量的平均值，即为信息熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息量：\n",
    "#### 意外越大，越不可能发生，概率就越小，信息量也就越大，也就是信息越多。比如说“今天肯定会天黑”，实现概率100%，说了和没说差不多，信息量就是0。\n",
    "#### 信息量= log2(1/概率)=log2(概率^-1)=-log2(概率)，log2是以2为底的对数。\n",
    "#### 举个例子：掷色子每个数有1/6的可能性，即log2(6)=2.6，1-6的全部可能性，二进制需要3位描述（3>2.6）；抛硬币正反面各1/2可能性，log(2)=1，二进制用一位即可描述，相比之下，掷色子信息量更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息熵是对于不确定性的度量而不是确定性的度量，信源是随机的，越是随机的信源，其信息熵就越大。**简单来说，信息熵就是信息杂乱程度的度量。熵越高，其包含的信息就越多；熵越低，其包含的信息就越少。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息熵采用概率分布的对数作为信息的度量原因在于可加性 $\\log$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （information entropy）熵的定义：是样本集合纯度最常用的一种指标\n",
    "#### 信息的样本集合可以看做：特征 类的数据集合\n",
    "#### 决策树学习的目的在于产生一颗泛化能力强，即处理未见示例能力强的决策树，基本流程遵循简单直观的“分而治之”(divide-and-conquer)策略。 \n",
    "#### 一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 熵=H=-sum(概率*log2(概率))，可以看到它是信息量的期望值，描述的也是意外程度，即不确定性。0<H<log2(m)，m是分类个数，log2(m)是均匀分布时的熵。二分类熵的取值范围是[0,1]，0是非常确定，1是非常不确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在样本集合D中，信息熵的第k类样本所占比列为：\n",
    "#### $p_k(k=1, k=2,...,k=|y|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 则D的信息熵（information entropy）为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $E(D) = - \\sum\\limits_{k=1}^{|y|}p_k\\log_2 p_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解信息熵的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结：\n",
    "* 在信息可能有N种情况时，如果每种情况出现的概率相等，那么N越大，信息熵就越大\n",
    "* 在信息可能有N种情况时，当N一定时，那么其中所有情况概率相等时信息熵是最大的；而如果有一种情况的概率比其他概率都大得多，那么信息熵就越小\n",
    "* 具体的值在具体的情况可以进行量化的计算比较（涉及条件熵）笼统来说：信息越确定，越单一，信息熵就越小 信息越不确定，越混乱，信息熵就越小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （information gain）信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益基本概念及公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 样本基本D中，特征（属性，离散属性）a中可能有V个取值\n",
    "#### $(a^1, a^2,...,a^V)$\n",
    "#### 使用特征a对于样本及D进行划分，则产生V个分支节点，其中第v个包含了D中所有在属性a上取值$a^V$的样本，记作$D^V$，即$D^V的信息熵为E(D^V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考虑到不同分支节点包含的样本数不同，则给分支节点赋权：$\\frac {D_v}{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最后可计算出属性a对于样本D划分所得到的信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Gain(D,a) = E(D) - \\sum\\limits_{v=1}^V \\frac{|D^V|}{|D|} E(D_V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3算法就是信息增益计算的 一般信息增益越大，意味着使用属性a进行划分所得的\"纯度提升\"越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例：相亲信息分析\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    年龄  学历 是否相亲\n",
      "0   25  大专    N\n",
      "1   33  大专    Y\n",
      "2   28  硕士    Y\n",
      "3   25  硕士    Y\n",
      "4   46  硕士    N\n",
      "5   40  本科    N\n",
      "6   34  硕士    Y\n",
      "7   36  本科    N\n",
      "8   35  硕士    Y\n",
      "9   30  本科    Y\n",
      "10  28  本科    N\n",
      "11  29  本科    Y\n",
      "[[25, '大专', 'N'], [33, '大专', 'Y'], [28, '硕士', 'Y'], [25, '硕士', 'Y'], [46, '硕士', 'N'], [40, '本科', 'N'], [34, '硕士', 'Y'], [36, '本科', 'N'], [35, '硕士', 'Y'], [30, '本科', 'Y'], [28, '本科', 'N'], [29, '本科', 'Y']]\n",
      "0.9798687566511527\n",
      "_____大专_____\n",
      "[[25, '大专', 'N'], [33, '大专', 'Y']]\n",
      "_____本科_____\n",
      "[[40, '本科', 'N'], [36, '本科', 'N'], [30, '本科', 'Y'], [28, '本科', 'N'], [29, '本科', 'Y']]\n",
      "_____硕士_____\n",
      "[[28, '硕士', 'Y'], [25, '硕士', 'Y'], [46, '硕士', 'N'], [34, '硕士', 'Y'], [35, '硕士', 'Y']]\n",
      "大专: 1.0\n",
      "本科: 0.9709505944546686\n",
      "硕士: 0.7219280948873623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10783596942530649"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据信息熵理论完成表中样本集合D的信息熵计算\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "degrees = ['大专', '大专', '硕士', '硕士', '硕士', '本科', '硕士', '本科', '硕士', '本科', '本科', '本科']\n",
    "dataSet = ['N', 'Y', 'Y', 'Y', 'N', 'N', 'Y', 'N', 'Y', 'Y', 'N', 'Y']\n",
    "age = [25, 33, 28, 25, 46, 40, 34, 36, 35, 30, 28, 29]\n",
    "\n",
    "dataSet = pd.DataFrame({'年龄':age, '学历': degrees, '是否相亲': dataSet}, columns=['年龄', '学历', '是否相亲'])\n",
    "print(dataSet)\n",
    "data = np.array(dataSet).tolist()\n",
    "print(data)\n",
    "len(data)\n",
    "def entropy_D(data):\n",
    "    k = len(data)\n",
    "    count = {}\n",
    "    for n in data:\n",
    "        p = n[-1]\n",
    "        if p not in count.keys():\n",
    "            count[p] = 0\n",
    "        count[p] += 1\n",
    "    ent = 0.0\n",
    "    for n in count:\n",
    "        prob = float(count[n])/k\n",
    "        ent -= prob * log(prob, 2)\n",
    "    return ent\n",
    "\n",
    "ent_D = entropy_D(data)\n",
    "print(ent_D)\n",
    "# 以学历为字段 拆分数据\n",
    "def splitdata(data, axis, keyword):\n",
    "    split_data = []\n",
    "    for n in data:\n",
    "        if n[axis] == keyword:\n",
    "            split_data.append(n)\n",
    "    return split_data\n",
    "set1 = splitdata(data, 1, '大专')\n",
    "set2 = splitdata(data, 1, '本科')\n",
    "set3 = splitdata(data, 1, '硕士')\n",
    "print('_____大专_____')\n",
    "print(set1)\n",
    "print('_____本科_____')\n",
    "print(set2)\n",
    "print('_____硕士_____')\n",
    "print(set3)\n",
    "print('大专:',entropy_D(set1))\n",
    "print('本科:', entropy_D(set2))\n",
    "print('硕士:', entropy_D(set3))\n",
    "# 计算字段学历的信息\n",
    "ent_D - (len(set1)/len(data)*entropy_D(set1) + len(set2)/len(data)*entropy_D(set2) + len(set3)/len(data)*entropy_D(set3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    年龄  学历 是否相亲\n",
      "0   25  大专    N\n",
      "3   25  硕士    Y\n",
      "2   28  硕士    Y\n",
      "10  28  本科    N\n",
      "11  29  本科    Y\n",
      "9   30  本科    Y\n",
      "1   33  大专    Y\n",
      "6   34  硕士    Y\n",
      "8   35  硕士    Y\n",
      "7   36  本科    N\n",
      "5   40  本科    N\n",
      "4   46  硕士    N\n",
      "[[25, '大专', 'N'], [28, '硕士', 'Y'], [25, '硕士', 'Y'], [28, '本科', 'N']]\n",
      "[[25, '大专', 'N'], [33, '大专', 'Y'], [28, '硕士', 'Y'], [25, '硕士', 'Y'], [46, '硕士', 'N'], [40, '本科', 'N'], [34, '硕士', 'Y'], [36, '本科', 'N'], [35, '硕士', 'Y'], [30, '本科', 'Y'], [28, '本科', 'N'], [29, '本科', 'Y']]\n",
      "0.010246088034509548\n",
      "25.5\n",
      "25.5\n",
      "28.5\n",
      "28.5\n",
      "29.5\n",
      "30.5\n",
      "33.5\n",
      "34.5\n",
      "35.5\n",
      "36.5\n",
      "40.5\n",
      "46.5\n"
     ]
    }
   ],
   "source": [
    "# 上述是根据离散型变量进行计算\n",
    "# 现根据连续型变量进行计算\n",
    "# 根据连续型变量计算 方法通常为在这个字段上找一个最佳分割点\n",
    "# 如果有n个数字 就有n-1种切法 如 [25 25] 就可以定切割点位25 [28 29]就可以定切割点28.5\n",
    "\n",
    "degrees = ['大专', '大专', '硕士', '硕士', '硕士', '本科', '硕士', '本科', '硕士', '本科', '本科', '本科']\n",
    "dataSet = ['N', 'Y', 'Y', 'Y', 'N', 'N', 'Y', 'N', 'Y', 'Y', 'N', 'Y']\n",
    "age = [25, 33, 28, 25, 46, 40, 34, 36, 35, 30, 28, 29]\n",
    "\n",
    "dataSet = pd.DataFrame({'年龄':age, '学历': degrees, '是否相亲': dataSet}, columns=['年龄', '学历', '是否相亲'])\n",
    "print(dataSet.sort_values(by='年龄'))\n",
    "# 这里从28 29中间分开\n",
    "set1 = np.array(dataSet[dataSet['年龄'] < 28.5]).tolist()\n",
    "set2 = np.array(dataSet[dataSet['年龄'] > 28.5]).tolist()\n",
    "age_ent = ent_D - (len(set1)/len(data)*entropy_D(set1) + len(set2)/len(data)*entropy_D(set2))\n",
    "# 切得不成功 信息增益太小了\n",
    "def cut(data, axis, num):\n",
    "    set1 = []\n",
    "    set2 = []\n",
    "    for n in data:\n",
    "        if n[axis] < num:\n",
    "            set1.append(n)\n",
    "        set2.append(n)\n",
    "    return set1, set2\n",
    "set1, set2 = cut(data, 0, 28.5)\n",
    "print(set1)\n",
    "print(set2)\n",
    "print(age_ent)\n",
    "age.sort()\n",
    "cut_age = []\n",
    "for n in age:\n",
    "    a = n\n",
    "    b = n+1\n",
    "    print((a+b)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "[['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'], ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'], ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '是'], ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘', '是'], ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑', '是'], ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '否'], ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘', '否'], ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', '否'], ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', '否'], ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '否'], ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '否'], ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '否'], ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '否'], ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '否']]\n"
     ]
    }
   ],
   "source": [
    "# 当所有属性都为离散型数据时\n",
    "# 计算其信息增益\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "file = 'xigua_data.txt'\n",
    "data = pd.read_csv(file, sep='\\t',index_col='编号', encoding='gbk')\n",
    "labels = list(data.columns)\n",
    "print(labels) # 特征 类\n",
    "dataSet = np.array(data).tolist() # 处理数据以列表形式 方便后续数据处理\n",
    "print(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975025463691153"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算根节点的信息熵\n",
    "def shonnonEnt(dataSet):\n",
    "    num_data = len(dataSet) # 样本集的总样本数量\n",
    "    # 建立投票机制\n",
    "    count = {}\n",
    "    for n in dataSet:\n",
    "        label = n[-1]\n",
    "        if label not in count.keys():\n",
    "            count[label] = 0\n",
    "        count[label] += 1\n",
    "    shonnonEnt = 0.0\n",
    "    for n in count:\n",
    "        prob = float(count[n]) / num_data # 投票之比\n",
    "        shonnonEnt -= prob * log(prob, 2) # 信息熵公式\n",
    "    return shonnonEnt\n",
    "shonnonEnt(dataSet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'],\n",
       " ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'],\n",
       " ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '是'],\n",
       " ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘', '否'],\n",
       " ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '否'],\n",
       " ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '否']]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据特征拆解数据集\n",
    "def splitdata(dataSet, axis, feature):\n",
    "    data = []\n",
    "    for n in dataSet:\n",
    "        if n[axis] == feature:\n",
    "            data.append(n)\n",
    "    return data\n",
    "set1 = splitdata(dataSet, 0, '青绿')\n",
    "set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'],\n",
       " ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'],\n",
       " ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘', '是'],\n",
       " ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑', '是'],\n",
       " ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '否'],\n",
       " ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '否']]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set2 = splitdata(dataSet, 0, '乌黑')\n",
    "set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'],\n",
       " ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', '否'],\n",
       " ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', '否'],\n",
       " ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '否'],\n",
       " ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '否']]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set3 = splitdata(dataSet, 0, '浅白')\n",
    "set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9182958340544896\n",
      "0.7219280948873623\n"
     ]
    }
   ],
   "source": [
    "print(shonnonEnt(set1))\n",
    "print(shonnonEnt(set2))\n",
    "print(shonnonEnt(set3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算信息增益及最优属性划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乌黑 的信息熵为: 0.9182958340544896\n",
      "浅白 的信息熵为: 0.7219280948873623\n",
      "青绿 的信息熵为: 1.0\n",
      "色泽 信息增益为: 0.10812516526536531\n",
      "_______________\n",
      "稍蜷 的信息熵为: 0.9852281360342516\n",
      "蜷缩 的信息熵为: 0.9544340029249649\n",
      "硬挺 的信息熵为: 0.0\n",
      "根蒂 信息增益为: 0.14267495956679288\n",
      "_______________\n",
      "沉闷 的信息熵为: 0.9709505944546686\n",
      "浊响 的信息熵为: 0.9709505944546686\n",
      "清脆 的信息熵为: 0.0\n",
      "敲声 信息增益为: 0.14078143361499584\n",
      "_______________\n",
      "清晰 的信息熵为: 0.7642045065086203\n",
      "模糊 的信息熵为: 0.0\n",
      "稍糊 的信息熵为: 0.7219280948873623\n",
      "纹理 信息增益为: 0.3805918973682686\n",
      "_______________\n",
      "平坦 的信息熵为: 0.0\n",
      "稍凹 的信息熵为: 1.0\n",
      "凹陷 的信息熵为: 0.863120568566631\n",
      "脐部 信息增益为: 0.28915878284167895\n",
      "_______________\n",
      "软粘 的信息熵为: 0.9709505944546686\n",
      "硬滑 的信息熵为: 1.0\n",
      "触感 信息增益为: 0.006046489176565584\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "num_feature = len(labels) - 1 # 计算子属性数量\n",
    "baseEntropy = shonnonEnt(dataSet) # 计算根节点信息熵\n",
    "features = labels[:-1] # 子属性\n",
    "for i in range(num_feature):\n",
    "    featlist = [example[i] for example in dataSet] # 按列提取数据\n",
    "    uniqueVals = set(featlist)\n",
    "    newEntropy = 0\n",
    "    for value in uniqueVals:\n",
    "        # 根据子属性及其取值划分样本集\n",
    "        subDataSet = splitdata(dataSet, i, value)\n",
    "        prob = len(subDataSet) / len(dataSet)\n",
    "        newEntropy += prob * shonnonEnt(subDataSet)\n",
    "        print(value,'的信息熵为:', shonnonEnt(subDataSet)) # 不同取值的信息熵\n",
    "    infoGain = baseEntropy - newEntropy # 计算信息增益\n",
    "    print(features[i],'信息增益为:', infoGain)\n",
    "    print('_______________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于信息增益选择最优划分属性\n",
    "def chooseBestfeatureToSplit_Gain(dataSet):\n",
    "    num_feat = len(dataSet[0]) - 1\n",
    "    baseEntropy = shonnonEnt(dataSet)\n",
    "    bestInfoGain = 0.0 #初始最优信息增益\n",
    "    bestFeature = -1 #初始最优子属性\n",
    "    for i in range(num_feat):\n",
    "        featlist = [example[i] for example in dataSet]\n",
    "        uniqueFeat = set(featlist)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueFeat:\n",
    "            subDataSet = splitdata(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * shonnonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "chooseBestfeatureToSplit_Gain(dataSet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 信息增益率——C4.5算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息增益准则对可数数目较多属性有所偏好。\n",
    "#### 考虑一个特殊情况，若分支数目就为样本数，则信息增益就为样本信息熵，此时信息增益亦亦越大，因此该情况下决策树不具有泛化能力，无法对新样本进行有效预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息增益率的公式为\n",
    "#### $Gain_ratio(D,a) = \\frac{Gain(D,a)}{IV(a)}$\n",
    "#### $IV(a) = -\\sum\\limits_{v=1}^{V}\\frac{|D^v|}{|D|}\\log_2 \\frac{|D^v|}{|D|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相比于ID3算法\n",
    "* 使用信息增益比例而非信息增益作为分裂标准\n",
    "* 处理含有带缺失值的样本方法为将这些值并入最常见的某一类中或以最常用的值代替\n",
    "* 处理连续值属性\n",
    "* 规则的产生：规则集存储于一个二维数组中，每一行代表决策树的一个规则\n",
    "* 交互验证：训练开始之前，预留一部分数据，训练之后，使用这部分数据对学习的结果进行验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于信息增益率选择最优划分属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chooseBestfeatureToSplit_GainRatio(dataSet):\n",
    "    num_feat = len(dataSet[0]) - 1\n",
    "    baseEntropy = shonnonEnt(dataSet)\n",
    "    bestGainRatio = 0.0 #初始最优信息增益\n",
    "    bestFeature = -1 #初始最优子属性\n",
    "    for i in range(num_feat):\n",
    "        featlist = [example[i] for example in dataSet]\n",
    "        uniqueFeat = set(featlist)\n",
    "        newEntropy = 0\n",
    "        iv = 0\n",
    "        for value in uniqueFeat:\n",
    "            subDataSet = splitdata(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            iv -= prob * log(prob, 2)\n",
    "            newEntropy += prob * shonnonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        GainRatio = infoGain / iv\n",
    "        if GainRatio > bestGainRatio:\n",
    "            bestGainRatio = GainRatio\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "chooseBestfeatureToSplit_GainRatio(dataSet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基尼指数（gini index)——CART决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 公式：\n",
    "#### $Gini(D) = 1- \\sum\\limits_{k=1}{|y|} P^2_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反映了从样本集D中随机抽取两个样本，其类别标记不一致的概率\n",
    "#### 因此$Gini(D)$越小，样本集D纯度越高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义属性a的基尼指数为\n",
    "#### $Gini.index(D) = \\sum\\limits_{v=1}^{|V|} \\frac{|D^v|}{|D|} Gini(D^v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在候选属性集中A中，选择使得划分后基尼指数最小的属性作为最优划分属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算基尼指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49826989619377154"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcGini(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "        Gini = 1.0\n",
    "        for key in labelCounts:\n",
    "            prob = float(labelCounts[key]) / numEntries\n",
    "            Gini -= prob * prob\n",
    "    return Gini\n",
    "\n",
    "calcGini(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于基尼指数选择最优属性划分\n",
    "def chooseBestFeatureToSplit_Gini(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    bestGini = 100000.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newGiniIndex = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitdata(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newGiniIndex += prob * calcGini(subDataSet)\n",
    "        if (newGiniIndex < bestGini):\n",
    "            bestGini = newGiniIndex\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "chooseBestFeatureToSplit_Gini(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
